% !TeX root=main.tex
% دستور زیر باید در اولین فصل شما باشد. آن را حذف نکنید!

\chapter{روش‌های یادگیری در داده‌های جریانی}
\thispagestyle{empty}
رده‌بندی فرایند یافتن یک مدل عمومی از داده‌های گذشته است به طوری که بتوان آن مدل را روی داده‌های جدید اعمال کرد. رده‌بندی از دو گام تشکیل شده است: گام یادگیری(آموزش) و گام تست. در بخش یادگیری سیستم تلاش می‌کند که یک مدل از مجموعه‌ی داده‌های نمونه‌ی مجموعه آموزشی یاد بگیرد؛ در گام تست از این مدل برای برچسب‌زنی به داده‌هایی که برچسب زده‌نشده‌اند استفاده می‌شود. در ادبیات داده‌های جریانی، تعداد زیادی الگوریتم رده‌بندی مانند درخت‌های تصمیم، رده‌بند بیزین، ماشین‌های بردار پشتیبان، k نزدیک‌ترین همسایگی و روش‌هایی که مجمعی از رده‌بندها را می‌سازند وجود دارد. در این فصل به بررسی این روش‌ها می‌پردازیم و روش کار هر کدام را به شکل مختصر توضیح می‌دهیم.

\section{درخت هافدین}\label{sec3}
درخت هافدین یک رده‌بند بر مبنای درخت تصمیم برای داده‌های جریانی است\cite{gama2007learning}. روش‌های سنتی درخت تصمیم، برای انتخاب خصیصه تقسیم نیاز به چندین بار پویش داده دارند که این موضوع در محیط داده‌های جریانی، عملا نشدنی است. سایر روش‌های رده‌بندی داده‌های جریانی نیز معمولا کاستی‌هایی مانند موارد زیر دارند:


\begin{itemize}
\item حساسیت زیاد به ترتیب نمونه‌ها
\item کارایی کم. در بعضی موارد آن‌های از الگوریتم‌های یادگیری دسته‌ای کندترند.
\end{itemize}
درخت هافدین که یکی از روش‌های جدید یادگیری برای جریان‌هاست چالش‌های زیر را حل می‌کند:

\begin{itemize}
\item عدم قطعیت در زمان یادگیری. زمان یادگیری در درخت هافدین برای هر نمونه ثابت است و این بدان معنیست که درخت هافدین برای کاوش در داده‌های جریانی مناسب است.
\item زمانی که تعداد نمونه کافی برای ساخت درخت پویش شود، نتیجه درخت‌ در روش هافدین تقریبا مشابه(یا برابر) با درخت‌های ساخته شده با روش‌های مرسوم یادگیری دسته‌ای است.
\end{itemize}

برای تعامل با چالش پویش چندباره داده‌ها از حد هافدین برای انتخاب یک خصیصه تقسیم بهینه، پس پویش تعداد کافی نمونه استفاده می‌شود. فرض کنید N تعداد مشاهدات مستقل از یک متغییر تصادفی r باشد که این متغییر تصادفی در محدود R و با میانگین $ \overline{r} $ است. حد هافدین تضمین می کند که میانگین درست r حداقل از $ \overline{r} -\epsilon $ با احتمال $ 1 - \delta $ بزرگ‌تر باشد. در این رابطه $ \delta $ پارامتری است که کاربر انتخاب می‌کند.

$$ P(E[r] \geq (\overline{r} - \epsilon)) \geq 1 - \delta, \epsilon = \sqrt{\frac{R^2ln(\frac{1}{\delta})}{2N}} $$


چیزی که حد هافدین را جالب توجه می‌کند، امکان گرفتن نتیجه‌های مشابه بدون در نظر گرفتن توزیع احتمالی‌ست که مشاهدات را تولید می‌کند. به عنوان نمونه اگر اختلاف بهره اطلاعاتی بین دو خصیصه A و B (که می‌دانیم بهره اطلاعاتی خصیصه A بیشتر از خصیصه B است) برابر با $0.3$ و $ e=0.1 $ باشد این معنی را می‌دهد که در آینده، تفاوت بین بهره اطلاعاتی A و B حداقل $ 0.3 - 0.1 = 0.2 $ است. به عبارت دیگر با احتمال $1 - \delta$، هنگامی که تفاوت بهره اطلاعاتی مشاهده شده بزرگ‌تر از  $ \epsilon $ باشد یک خصیصه در مقایسه با دیگر خصیصه‌ها قالب است.

فرض کنیم $ G(x_{i}) $ یک معیار اکتشافی برای انتخاب کردن خصیصه جداسازی باشد و پس از مشاهده N نمونه، $x_{a}$ و $x_{b}$ اولین و دومین خصیصه برتر برای خصیصه جداسازی باشند. با این مفروضات، در درخت هافدین، متغییر r از رابطه $ r = \triangle G = G(X_a) - G(X_b) $ تعیین می‌شود.
 اگر $ \overline{r} = \triangle \overline{G} = \overline{G}(X_a) - \overline{G}(X_b) $ باشد که در آن $\epsilon $ از رابطه بالا محاسبه شده است،
می‌گوییم با اطمینان $1 - \delta $ این تفاوت بزرگتر از $ \overline{r} - \epsilon > 0 $ است. بنابراین خصیصه $x_{a}$ برای جداسازی انتخاب می‌شود.


نویسندگان الگوریتم درخت هافدین را توسط یک یادگیرنده با نام «درخت تصمیم خیلی سریع»(VFDT) پیاده‌سازی کرده‌اند. این پیاده‌سازی شامل بهبودهایی برای استفاده‌های خاص، مانند راهبرد محدود کردن گره، راهندازی سریع با استفاده از یادگیرنده‌های مبتنی بر RAM و قابلیت بازپویش نمونه‌های قبلی زمانی که سرعت گذر داده کم است می‌باشد.
\\
الگوریتم درخت هافدین،‌ یک الگوریتم با دقت بالاست که می‌تواند بسیار خوب با مجموعه‌ی داده‌های بزرگ کار کند، ولی این الگوریتم نمی‌تواند با مساله رانش مفهوم در داده‌های جریانی تعامل کند\cite{Nguyen2015}. الگوریتم CVFDT \cite{hulten2001mining} یک تعمیم از الگوریتم درخت هافدین است که برای تعامل با رانش مفهوم در داده‌های جریانی  سازگار شده است. CVFDT ، بعضی آماره‌های کارآمد را برای بررسی کردن اعتبار تصمیم‌های قبلی در کنار هر گره درخت نگهداری می‌کند. زمانی که یک داده وارد می‌شود، به طور مداوم این آماره‌های قرارگرفته در کنار گره‌های درخت بروزرسانی می‌شوند. با کمک پنجره‌های جابه‌جا شونده روی داده، این الگوریتم اثر داده‌هایی که از پنجره خارج هستند را در آماره‌های هر گره نادیده می‌گیرد. این پویش دوره‌ای گره‌های درخت‌ باعث تشخیص رانش مفهوم می‌شوند. اگر رانش مفهوم ظاهر شده باشد، CVFDT یه صورت موازی شاخه‌های جایگزین را با انتخاب بهترین خصیصه‌های جدید و حذف شاخته‌های قدیمی، اگر دقتشان کم باشد، گسترش می‌دهد.

\section{رده‌بند بیزین}

سیدل
\LTRfootnote{Seidl}
و همکارانش \cite{seidl2009indexing} یک روش جالب برپایه نمایه‌سازی
\LTRfootnote{Index-Based}
رده‌بند ارایه داده‌اند که درخت بیز نامیده می‌شود. درخت بیز از یک درخت مخلوط-گوسین سلسله مراتبی
\LTRfootnote{Hierarchical Gaussian-mixture Tree}
جهت نمایان‌کردن تمام مجموعه‌ی داده استفاده می‌کند. هر گره از درخت شامل آماره‌های از نمونه‌های داده مثل مستطیل محدود‌کننده کمینه
\LTRfootnote{Minimum Bounding Rectangle}
، تعداد نمونه‌های داده، جمع خطی و مجموع مربعات تمام داده است.
برای حل مساله‌ی رده‌بندی چند کلاسه، یک درخت بیز برای هر کلاس ساخته‌ می‌شود.برای هر داده‌ی ست مانند x    ، الگوریتم سعی می‌کند که نزدیک‌ترین مجموعه گره‌ها را که مرز نامیده می‌شوند پیدا کند. احتمال تعلق x به کلاس $c_i$ توسط رابطه زیر محاسبه می‌شود:

$$
P(c_i | x ) = 
\left(       
\sum_{e_s \in E_i} \frac{n_{e_s}}{n}
g(x, \mu_{e_s}, \sigma_{e_s})
\right)
\star P(c_i)/P(x)
$$


که در این رابطه $e_s$ یک گره درخت در مخموعه مرز $E_i$ است. $n_{e_s}$ و $ \mu_{e_s}$ و $\sigma_{e_s}$ به ترتیب، تعداد نمونه‌های داده،‌مرکز و انحراف گره $e_s$ هستند. برای مشخص کردن برچسب نمونه‌ی x ، احتمال برای تمام کلاس‌ها حساب می‌شود و کلاسی که بیشترین احتمال را داشته‌باشد به عنوان کلاس نمونه x اعلام می‌شود. درخت بیز یک رده‌بند است که پس از مدت کمی از شروع، می‌تواند نتیجه و تصمیم داشته باشد و سپس آن دقت‌ها را با انتخاب مرز‌های دقیق‌تر افزایش دهد.


\section{شبکه عصبی}
لیت
\LTRfootnote{Leite}
و همکارانش \cite{leite2010evolving} روش شبکه‌های عصبی داده‌دانه تغییر پذیر
\LTRfootnote{Evolving Granular Neural Network}
(eGNN)
را جهت رده‌بندی داده‌های جریانی ارایه داده‌اند. دو گام در eGNN وجود دارد؛ در گام اول، eGNN از نرون‌های T-S برای ساختن ریزدانه‌های اطلاعات برای داده‌هایی که درحال آمدن هستند، استفاده می‌کند؛ در گام دوم، شبکه عصبی روی این ریزدانه‌های اطلاعاتی، به جای داده‌ی اصلی، ساخته می‌شود. یک ریزدانه مرتبط با برچسب یک کلاس به شکل یک تابع عضویت سه‌گوش
\LTRfootnote{Triangular membership function}
 تعریف می شود که بعدا جهت تطبیق با داده‌های جدید تغییر می‌کند.
 
  وزن‌ها با یک ثابت زوال
\LTRfootnote{Decay}
کاهش پیدا می‌کنند؛ این پردازش کمک می‌کند که درجه اهمیت ریزدانه‌های منقضی شده کاهش پیدا کند. به شکل پایه، eGNN از نمونه‌های کلاس در فرم ریزدانه‌های اطلاعاتی جهت انجام رده‌بندی استفاده می‌کند. زمانی که داده‌های تست می‌آیند یک نرون بیشینه‌گیری، بهترین پیش‌بینی ریزدانگی را انتخاب می‌کند و برچسب آن را به نمونه، جهت پیشبینی کلاسش، اختصاص می‌دهد. eGNN می‌تواند در مساله رده‌بندی در محیط‌های که دایما تغییر می‌کند استفاده شود اما نیاز به زمان طولانی آموزش، هنوز یک محدودیت برای شبکه‌های eGNN به منظور کار با داده‌های بسیار بزرگ است. البته، در قسمت آزمایش‌های مقاله اصلی، مجموعه‌های داده‌ی کوچک برای ارزیابی این روش استفاده شده است؛ سپس نویسنده، این روش‌ را جهت کارایی بهتر و کار به شکل نیمه نظارتی \LTRfootnote{Semi-Supervised} گسترش داده‌است تا با مجموعه‌ی داده‌هایی که کاملا برچسب گذاری نشده‌اند هم کار کند. 

\section{ماشین‌های بردار پشتیبان}
ماشین‌های بردار پشتیبان
\LTRfootnote{Support Vector Machine}
کارایی خود را در بسیاری از مساله‌های یادگیری ماشین با داده‌های ایستا نشان داده‌‌اند\cite{Nguyen2015}. البته استفاده از ماشین‌های بردار پشتیبان برای کاربردهایی با مقیاس بزرگ بسیار پرهزینه است. اگر N تعداد نمونه‌های داده باشد، این الگوریتم از پیچیدگی زمانی $ O(N^3) $ و از پیچیدگی حافظه‌ای $ O(N^2) $ است. برای کار کردن با داده‌های بسیار بزرگ، تی‌سنگ
\LTRfootnote{Tsang}
و همکارانش، الگوریتم ماشین بردار هسته
\LTRfootnote{Core Vector Machine}
(CVM)
 را ارایه داده‌اند تا پیچیدگی الگوریتم ماشین بردار پشتیبان را کاهش دهند. این الگوریتم از حداقل گوی نزدیک(MEB) برای این کار استفاده می‌کند. یک MEB ابر کره
\LTRfootnote{Hypersphere}
(کره با بیش از سه بعد) است که نماینده یک مجموعه از نمونه‌های داده کنار آن است\cite{tsang2007simpler} . این الگوریتم ابتدا MEB های نماینده را که تخمین خوبی از داده اصلی باشند، پیدا می‌کند. سپس مساله بهینه‌سازی پیدا کردن محدوده‌ی بیشینه بر روی این مجموعه‌های MEB اعمال می‌شود.


ری
\LTRfootnote{Rai}
و همکارانش  \cite{rai2009streamed} الگوریتم دیگری با نام StreamSVM که یک بهبود از الگوریتم CVM است را برای کار با داده‌های جریانی ارایه کرده‌اند. در این الگوریتم، یک MEB شعاع انعطاف پذیری است، هرزمان نمونه‌های جدید آموزش اضافه می‌شوند، این شعاع افزایش می‌یابد. این الگوریتم زمانی که تخمین‌ها درست باشند با بهینه‌ترین روش‌ها قابل رقابت است، اما StreamSVM هنوز قابلیت تشخیص رانش‌ مفهوم در داده‌های جریانی را ندارد.


\section{رده‌بند نزدیک‌ترین همسایگی}

در مورد خوشه بندی جریان‌های داده‌ای(که مورد بحث این گزارش نیست)، یک روش خوشه‌بندی با نام CluStream وجود دارد. این روش از تعریفی تحت عنوان ریز-خوشه
\LTRfootnote{Micro-Cluster}
برای خوشه‌بندی داده‌ها استفاده می‌کند. یک ریز-خوشه برای تعدادی داده، شامل آماره‌هایی نظیر مجموع مربعات نقاط، مجموع نقاط، مجموع مربعات زمان رخداد نقاط، مجموع خطی زمان رخداد و تعدا نمونه‌هاست. این روش خوشه‌بندی به جای ذخیره کل داده‌های خوشه، در گام آنلاین ریز-خوشه‌های آن‌ها که حجم کمتری را اشغال می‌کند و پردازش آن آسان‌تر است را ساخته و ذخیره می‌کند و در گام آفلاین به جای خوشه‌بندی کل داده‌ها، این ریز-خوشه‌ها را خوشه‌بندی می‌کند\cite{Nguyen2015}.
روش On-Demand-Stream \cite{aggarwal2003framework} یک گسترش از روش CluStream است که همانند این روش، از رویکرد آنلاین-آفلاین و پنجره یک‌بر استفاده می‌کند. تفاوت ریز-خوشه در این روش با ریز-خوشه در روش CluStream این است که برچسب کلاس نیز به ریز-خوشه اضافه می‌شود و ریز-خوشه‌ها فقط شامل نمونه‌هایی از یک کلاس مشخص و یکسان هستند. در رده‌بندی آفلاین، سعی می‌شود که بهترین پنجره از نمونه‌های داده که افق زمان
\LTRfootnote{Time Horizon}
نامیده می‌شود انتخاب شود و ریز-خوشه‌ها از این افق زمانی استخراج می‌شوند. روش On-Demand-Stream از یک رده‌بندی 1NN برای نسبت‌دادن داده‌ی آزمایش به نزدیک‌ترین ریز-خوشه استفاده‌ می‌کند.

با الهام‌گرفتن از ایده M-tree، ژنگ و همکارانش  \cite{zhang2011enabling} یک روش ساختار Lazy-tree برای نمایه سازی ریز-خوشه‌ها پیشنهاد داده‌اند. این کمک می‌کند که پیچیدگی زمانی روش‌ نزدیک‌ترین همسایگی اگر فرض کنیم N تعداد ریز-خوشه‌ها در Lazy-tree باشد، از $O(N)$ به $ O(log(N))$ کاهش یابد. درخت شامل سه عملگر اصلی است: جستجو، حذف و افزودن. اضافه‌کردن و حذف‌کردن برای افزودن گره‌های جدید و حذف‌کردن گره‌های منقضی شده به کار‌ می‌رود و تضمین می‌کنند که درخت متعادل بماند. عملگر جستجو برای رده‌بندی داده‌های تست استفاده می‌شود.


\section{مجمع رده‌بندها}
روش‌های
\lr{Bagging $ \& $ Boosting}
برتری خود را از طریق آزمایش‌ها روی مجموعه‌ی داده‌های سنتی اثبات کرده‌ است، نظر به این موضوع، بسیاری از محققین تلاش کرده‌اند که این روش‌ها را برای کار روی داده‌های جریانی سازگار کنند.
اوزا
\LTRfootnote{Oza}
و همکارانش \cite{oza2005online} روشی تحت عنوان
\lr{Bagging $ \& $ Boosting}
آنلاین ارایه کرده‌اند که یکی از اولین‌ کار‌ها برای سازگار کردن این روش بوده است. با دید آماری، هر نمونه از داده‌های آموزشی k بار در مجموعه‌‌ی آموزشی اعضای رده‌بند با احتمال زیر مشاهده می‌شود:

$$
P(k) = \left(\begin{array}{c}N\\ k\end{array}\right)
\left(\frac{1}{N} \right)^{k}
\left(1 - \frac{1}{N} \right)^{N-k}
$$
که در رابطه k اندازه مجموعه‌ی آموزشی و N اندازه مجموعه‌ی داده‌هاست. در داده‌های جریانی می‌توان فرض کرد که تعداد نمونه‌های داده نامحدود است، لذا $N \rightarrow \infty$؛ بنابراین احتمال $ P(k) $ به توزیع احتمال پوآسون میل می‌کند که رابطه آن مشابه زیر است:
$$Poisson(1) = exp(−1)/k!$$
با این مشاهده، اوزا و همکاران روش
\lr{Online Bagging}
را جهت نسبت دادن هر شی از داده به یک وزن متناسب با توزیع پواسون ارایه کردند. در
\lr{Online Boosting} 
وزن‌های داده‌هایی که در حال آمدن هستند و اعضای رده‌بندها بر اساس نسبت خطای اعضای رده‌بند به خطای کنونی تنظیم می‌شود.
\subsection{مجمع وزنی رده‌بند‌ها}
نظر به منقضی شدن داده‌های قدیمی، باید به توزیع داده به جای زمان رسیدن داده اعتماد کرد. بر این اساس ونگ
\LTRfootnote{Wang}
و همکارانش \cite{wang2003mining} یک روش دقت-وزن‌ برای مجمع رده‌بندها
\LTRfootnote{Accuracy-Weighted Ensemble}
(AWE)
ارایه کردند تا بتوان رانش مفهوم را در داده‌های جریانی کاوش کرد. الگوریتم با تعداد k رده‌بند که می‌توانند $RIPPER$ یا $C4.5$ یا بیزین ساده باشند ایجاد می‌شود و با همان تعداد نیز نگهداری می‌شود. با استفاده از یک تکه از نمونه‌های داده‌ای که در حال آمدن هستند یک رده‌بند جدید آموزش داده می‌شود؛ سپس مجمع با انتخاب k امین رده‌بندهای با دقت سازمان‌دهی می‌شود و وزن هر رده‌بند متناسب با دقت آن تعیین می‌شود. این روش بهتر از یک رده‌بند تنهای داده‌های جریانی مانند VFDT و CVFDT کار می‌کند ولی دقت‌ آن، خیلی به اندازه تکه‌ی انتخاب شده  و تعداد رده‌بندها (k) وابسته است.

در کاربردهای واقعی، در داده‌های جریانی ممکن است خطاهایی نظیر این که داده‌ها اشتباه برجسب خورده‌ یا مقدار اشتباه داشته باشند دیده شود. ژنگ
\LTRfootnote{Zhang}
و همکارانش \cite{zhang2011robust} الگوریتم مجمع متراکم را برای تعامل با داده‌های دارای خطا ارایه کردند. این رویکرد ترکیبی از چهارچوب‌های افقی و عمودی مجمع‌هاست؛ چهارچوب افقی رده‌بندهای مخلتفی بر روی هر چانک داده می‌سازد در حالی که چهارچوب عمودی رده‌بندهای مختلفی روی چانک داده‌ی بروز، با استفاده از الگوریتم‌های مختلف می‌سازد. چهارچوب افقی در برابر نویز مقاوم است و می‌تواند از اطلاعات گذشته استفاده کند ولی وقتی که رانش مفهومی رخ دهد(مفهوم در داده‌های جریانی به طور عمده تغییر کند) مناسب نیست. از طرف دیگر، چهارچوب عمودی می‌تواند وقتی که رانش مفهوم رخ دهد نتیجه خوبی بدهد ولی در برابر نویز حساس است. ساختن رده‌بندها روی چانک‌های مختلف داده و با استفاده از الگوریتم‌های مختلف یادگیری، می‌تواند راه خوبی برای جمع‌آوری یک مجمع از رده‌بندها باشد. این رده‌بندها در یک ماتریس با نام ماتریس رده‌بند
\LTRfootnote{Classifier Matrix}
جمع‌اوری می‌شوند. روش میانگین‌گیری وزنی روی این ماتریس، برای پیش‌بینی برچسب داده‌های آزمایش استفاده می‌شود. نویسنده به شکل تئوری اثبات کرده است که جمع‌‌آوری یک مجمع، به طور میانگین مجموع مربعات خطای کمتر یا حداکثر برابر با هر کدام از چهارچوب‌های افقی یا عمودی (به تنهایی) دارد.
\\


نگوین
\LTRfootnote{NGUyen}
و همکارانش \cite{nguyen2012heterogeneous} روی مساله یادگیری داده‌های جریانی با ابعاد بالا زمانی که فقط یک زیر مجموعه از ویژگی‌ها برای فرایند یادگیری با اهمیت هستند کار کرده‌اند. در این موضوع، مفهوم ویژگی‌ مرتبط یک مفهوم موقت و محدود به یک دوره زمانی خاص است. ویژگی‌های آموزنده
\LTRfootnote{Informative}
، ممکن است پس از مدتی نامربوط شوند در حالی که ویژگی‌های به‌درد نخور به ویژگی‌های مهم تبدیل شوند. نویسنده با ارایه مفهوم رانش ویژگی، به طور مثال تغییر در مجموعه‌ی ویژگی‌های باارزش، و ادغام این مفهوم با وزن‌دهی در مجمع رده‌بند ها، سعی کرده است که این مساله را حل کند. روش انتخاب ویژگی‌های چند متغییره، سازگار با تکنیک پنجره کشویی است و می‌توان به کمک آن رانش‌های ویژگی را تشخیص داد.
وقتی یک رانش تدریجی رخ‌ می‌دهد، رده‌بندهای مجمع بروزرسانی می‌شوند و وزن آن‌ها با توجه به نسبت خطایشان تنظیم می‌شود. زمانی که یک رانش ویژگی رخ می‌دهد، مجمع یادگیرنده‌های که قبلا استفاده می‌شده را با رده‌بندهایی که بروز رسانی شده‌اند جایگزین می‌کند. در بروزرسانی، یادگیری با یک مجموعه‌ی جدید از ویژگی‌های باارزش انجام می‌شود. آزمایش‌ها نشان می‌دهد که برای داده‌های جریانی با ابعاد بالا، این روش موثر و کارآمد است.
\subsection{درخت‌های سازگار تصمیم مبتنی بر رویکرد یکی در برابر بقیه }
درخت سازگار تصمیم مبتنی بر رویکرد یکی در برابر بقیه \LTRfootnote{Adapted One-vs-All Decision Trees (OVA)} یک روش مجمع جدید برای داده‌های جریانی است\cite{hashemi2009adapted}. در این روش، مجمع k رده‌بند باینری CVFDT را یادمی‌گیرد. هر رده‌بند آموزش‌ داده شده است که تشخیص دهد یک نمونه به یک کلاس خاص متعلق است یا به باقی کلاس‌ها. برای رده‌بندی یک داده‌ی جدید، همه رده‌بندها اجرا می‌شوند و رده‌بندی که مطمن‌ترین نتیجه را داد، به عنوان برچسب نمونه‌ی جدید انتخاب می‌شود. اطمینان رده‌بندی متناسب با کلاس غالب در برگ‌ها زمانی که نمونه تست وارد می‌شود تنظیم می‌شود. برای دستیابی به بیش‌ترین دقت‌ها، OVA تلاش می‌کند یک مجمعی از رده‌بندهای CVFDT بسازد که کمترین همبستگی‌ خطا و بیشترین تنوع را داشته باشند. این روش به سهولت و سریع با رانش‌ مفهوم سازگار می‌شود و فقط کافیست دو بخش رده‌بندها که مرتبط با کلاسی که تغییر کرده است، بروز رسانی شوند؛ به علاوه این روش به خوبی برای داده‌های جریانی نامتوازن کار می‌کند.


\subsection{مجمع متا-دانش}

بیشترین حد ممکن پیچیدگی برای رده‌بندهای یادگیرنده، محدود به محدویت‌های زمانی جریان‌های داده‌ای در کاربردهای دنیای واقعی است. ژنگ
\LTRfootnote{Zhang}
و همکارانش، یک روش مجمع  متا-دانش
\LTRfootnote{Meta-knowledge Ensemble}
که بهترین و مناسب‌ترین رده‌بند ها را برای رده‌بندی داده‌های تست انتخاب می‌کند را ارایه دادند\cite{zhang2011enabling}.
یک مجمع درخت برای سازماندهی رده‌های پایه ساخته شده است و هر‌ کدام یک وزن و یک محدوده از کل داده‌ها اختصاص داده می‌شود. مشابه RTree ، درخت مجمع‌ها سه عملگر پایه، شامل جستجو، اضافه کردن و حذف کردن دارد. ارتفاع این درخت متوازن است و پیچیدگی زمانی لگاریتمی را برای پیش‌بینی تضمین می‌کند. از عملگر افزودن برای ادغام یک رده‌بند جدید به مجمع استفاده می‌شود. زمانی که تعداد رده‌بندها در یک گره، از یک حد از پیش تعیین شده تجاوز کند، گره با این ایده که محدوده تحت پوشش هر گره کمینه شود،‌ به دو گره تقسیم‌ می‌شود. زمانی که ظرفبت E-tree پر شده باشد، عملگر حذف رده‌بندهای منقضی شده را حذف می‌کند و درخت ممکن است که دوباره سازماندهی شود تا متعادل بودن تضمین شود.
عملگر جستجو نیز برای رده‌بندی یک نمونه مانند x استفاده می‌شود. رده‌بندهای فضای نزدیک شامل x فراخوانی می‌شوند و یک روش رای‌گیری وزنی برای تصمیم‌گیری برچسب نمونه‌ی x اجرا می‌شود.

